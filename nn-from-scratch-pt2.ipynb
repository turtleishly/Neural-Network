{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "584601a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "data = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3fb5cc3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training data management\n",
    "data= np.array(data)\n",
    "\n",
    "#Train test split 80:20\n",
    "test_datas = data[int(len(data)*0.8):]\n",
    "train_datas = data[:int(len(data)*0.8)]\n",
    "\n",
    "#Separating pixel data and label data\n",
    "train_labels = train_datas[:,0] #label col\n",
    "train_datas = (train_datas[:,1:] - np.min(train_datas[:,1:]))/(np.max(train_datas[:,1:])-np.min(train_datas[:,1:])) # pixel data, scaled to 0-1\n",
    "\n",
    "test_labels = test_datas[:,0] #label col\n",
    "test_datas = (test_datas[:,1:] - np.min(test_datas[:,1:]))/(np.max(test_datas[:,1:])-np.min(test_datas[:,1:])) # pixel data, scaled to 0-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0ca5dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x): #sigmoid func to squish all inputs into range 0 to 1\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5cb0326a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialization\n",
    "\n",
    "size=[16, 10]\n",
    "train_data = train_datas[:10] \n",
    "train_label = train_labels\n",
    "# --------------------------------------\n",
    "\n",
    "weights = [] #list to store all the weights for every layer\n",
    "biases = [] #list to store all the biases for every layer\n",
    "\n",
    "#Randomly initialize weights and biases to append to list\n",
    "'''\n",
    "weights.append(np.random.uniform(-0.1,0.1,size=(size[0],len(train_data[0])))) #First layer\n",
    "biases.append(np.random.uniform(-0.1,0.1,size[0])) \n",
    "for i in range(len(size)-1): \n",
    "    weights.append(np.random.uniform(-0.1,0.1,size=(size[i+1],size[i]))) #following layers\n",
    "    biases.append(np.random.uniform(-0.1,0.1,size[i+1])) \n",
    "'''\n",
    "\n",
    "#Try using Xavier/Glorot initialization\n",
    "for i in range(len(size)): #Initialize weights for each layer\n",
    "    if i == 0:\n",
    "        weights.append(np.random.randn(size[0], len(train_data[0])) * np.sqrt(1/len(train_data[0])))\n",
    "    else:\n",
    "        weights.append(np.random.randn(size[i], size[i-1]) * np.sqrt(1/size[i-1]))\n",
    "\n",
    "for i in range(len(size)):  #Initialize biases for each layer\n",
    "    if i == 0:\n",
    "        biases.append(np.zeros(size[0])) #First layer biases\n",
    "    else:\n",
    "        biases.append(np.zeros(size[i]))  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f0ac5c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights, iteration 0\n",
      "[1.30862550e-05 1.77499735e-05 6.54312749e-06 0.00000000e+00]\n",
      "\n",
      " [ 0.0467435  -0.03802533  0.02486913  0.06770515]\n",
      "\n",
      " [-0.0071969  -0.0054507  -0.0046609  -0.00588062 -0.00549288 -0.00556801\n",
      " -0.00643707 -0.00658048 -0.00552199 -0.00539419 -0.00576723 -0.00547148\n",
      " -0.00584958 -0.0045769  -0.00574469 -0.00492983]\n",
      "\n",
      " [-0.47730139  0.17890558  0.48633069 -0.19608    -0.12501675 -0.06667329\n",
      "  0.0677023  -0.30814525 -0.03055555 -0.04629964 -0.37025132  0.20639895\n",
      "  0.56576327  0.25252925  0.07828255 -0.0693977 ]\n",
      "biases, iteration 0\n",
      "\n",
      " [ 5.07614219e-05 -4.44027895e-04 -2.89423996e-04  1.65995244e-04\n",
      "  1.93724464e-04 -1.44972201e-04 -3.29163432e-04 -1.32856664e-04\n",
      " -2.77981933e-04  3.93367722e-05  1.64248949e-04 -6.34226906e-05\n",
      "  4.85612341e-04 -4.00647835e-04 -2.24374585e-04  1.17076355e-04]\n",
      "\n",
      " [ 5.07614219e-05 -4.44027895e-04 -2.89423996e-04  1.65995244e-04\n",
      "  1.93724464e-04 -1.44972201e-04 -3.29163432e-04 -1.32856664e-04\n",
      " -2.77981933e-04  3.93367722e-05  1.64248949e-04 -6.34226906e-05\n",
      "  4.85612341e-04 -4.00647835e-04 -2.24374585e-04  1.17076355e-04]\n",
      "\n",
      " [-0.01123337 -0.01862826 -0.0254413  -0.00839996 -0.02376801 -0.01126143\n",
      " -0.01657778 -0.00986424 -0.0294144  -0.00945394]\n",
      "\n",
      " [-0.01123337 -0.01862826 -0.0254413  -0.00839996 -0.02376801 -0.01126143\n",
      " -0.01657778 -0.00986424 -0.0294144  -0.00945394]\n",
      "weights, iteration 1\n",
      "[1.57515226e-05 2.13650971e-05 7.87576128e-06 0.00000000e+00]\n",
      "\n",
      " [ 0.04675925 -0.03800397  0.024877    0.06770515]\n",
      "\n",
      " [-0.0067381  -0.005141   -0.00424231 -0.00559324 -0.00516449 -0.00525104\n",
      " -0.00602693 -0.0061975  -0.00511043 -0.00506053 -0.00545805 -0.00509243\n",
      " -0.00547397 -0.00421937 -0.00537223 -0.00453092]\n",
      "\n",
      " [-0.48403949  0.17376458  0.48208838 -0.20167324 -0.13018123 -0.07192433\n",
      "  0.06167537 -0.31434275 -0.03566598 -0.05136017 -0.37570937  0.20130651\n",
      "  0.5602893   0.24830988  0.07291032 -0.07392862]\n",
      "biases, iteration 1\n",
      "\n",
      " [ 9.35607764e-05 -4.01729763e-04 -2.45363536e-04  1.94793057e-04\n",
      "  2.29320296e-04 -1.20956917e-04 -3.07479941e-04 -1.20554452e-04\n",
      " -2.37033891e-04  8.01981142e-05  1.83150159e-04 -3.12013469e-05\n",
      "  4.97329936e-04 -3.84979036e-04 -1.96174895e-04  1.34601459e-04]\n",
      "\n",
      " [ 1.44322198e-04 -8.45757659e-04 -5.34787533e-04  3.60788302e-04\n",
      "  4.23044760e-04 -2.65929118e-04 -6.36643372e-04 -2.53411116e-04\n",
      " -5.15015823e-04  1.19534886e-04  3.47399108e-04 -9.46240375e-05\n",
      "  9.82942277e-04 -7.85626871e-04 -4.20549480e-04  2.51677814e-04]\n",
      "\n",
      " [-0.01051203 -0.01767302 -0.02381008 -0.00784852 -0.02447371 -0.01043494\n",
      " -0.01528172 -0.00924031 -0.02959458 -0.00887095]\n",
      "\n",
      " [-0.0217454  -0.03630128 -0.04925138 -0.01624848 -0.04824171 -0.02169637\n",
      " -0.03185951 -0.01910455 -0.05900898 -0.01832489]\n",
      "weights, iteration 2\n",
      "[1.80110535e-05 2.44298864e-05 9.00552674e-06 0.00000000e+00]\n",
      "\n",
      " [ 0.04677726 -0.03797954  0.02488601  0.06770515]\n",
      "\n",
      " [-0.00629979 -0.00484849 -0.00384658 -0.00531572 -0.00484769 -0.0049485\n",
      " -0.00563722 -0.00583238 -0.00472095 -0.00474089 -0.00516042 -0.00473026\n",
      " -0.00510578 -0.00388183 -0.00501843 -0.00414972]\n",
      "\n",
      " [-0.49033928  0.16891609  0.4782418  -0.20698896 -0.13502892 -0.07687283\n",
      "  0.05603815 -0.32017513 -0.04038693 -0.05610106 -0.38086979  0.19657626\n",
      "  0.55518352  0.24442805  0.0678919  -0.07807834]\n",
      "biases, iteration 2\n",
      "\n",
      " [ 1.30861595e-04 -3.60475066e-04 -2.03971072e-04  2.19892586e-04\n",
      "  2.62037872e-04 -9.73545686e-05 -2.80806865e-04 -1.03686998e-04\n",
      " -1.96976389e-04  1.16101176e-04  2.00290471e-04 -1.21724515e-07\n",
      "  5.05831526e-04 -3.63082767e-04 -1.70337950e-04  1.52602874e-04]\n",
      "\n",
      " [ 2.75183793e-04 -1.20623272e-03 -7.38758605e-04  5.80680888e-04\n",
      "  6.85082632e-04 -3.63283687e-04 -9.17450237e-04 -3.57098114e-04\n",
      " -7.11992212e-04  2.35636062e-04  5.47689579e-04 -9.47457620e-05\n",
      "  1.48877380e-03 -1.14870964e-03 -5.90887430e-04  4.04280688e-04]\n",
      "\n",
      " [-0.00981994 -0.01665585 -0.0221407  -0.00734084 -0.02496289 -0.00968522\n",
      " -0.01411513 -0.00867068 -0.02929271 -0.00834029]\n",
      "\n",
      " [-0.03156534 -0.05295713 -0.07139208 -0.02358932 -0.0732046  -0.03138159\n",
      " -0.04597464 -0.02777523 -0.08830169 -0.02666518]\n",
      "weights, iteration 3\n",
      "[1.98265469e-05 2.68923907e-05 9.91327345e-06 0.00000000e+00]\n",
      "\n",
      " [ 0.04679708 -0.03795265  0.02489592  0.06770515]\n",
      "\n",
      " [-0.00588369 -0.00457394 -0.0034745  -0.0050496  -0.00454394 -0.00466178\n",
      " -0.00526964 -0.00548711 -0.00435455 -0.00443645 -0.0048761  -0.00438638\n",
      " -0.00474785 -0.00356543 -0.00468455 -0.00378797]\n",
      "\n",
      " [-0.49622297  0.16434216  0.4747673  -0.21203856 -0.13957286 -0.08153461\n",
      "  0.05076852 -0.32566224 -0.04474148 -0.06053751 -0.38574589  0.19218988\n",
      "  0.55043566  0.24086262  0.06320735 -0.0818663 ]\n",
      "biases, iteration 3\n",
      "\n",
      " [ 1.61890248e-04 -3.20680755e-04 -1.65533623e-04  2.40442161e-04\n",
      "  2.90754359e-04 -7.43495690e-05 -2.49412867e-04 -8.30300332e-05\n",
      " -1.58061387e-04  1.46381657e-04  2.15409565e-04  2.89871927e-05\n",
      "  5.10531260e-04 -3.35340846e-04 -1.46116976e-04  1.70388434e-04]\n",
      "\n",
      " [ 4.37074041e-04 -1.52691348e-03 -9.04292229e-04  8.21123049e-04\n",
      "  9.75836991e-04 -4.37633256e-04 -1.16686310e-03 -4.40128147e-04\n",
      " -8.70053599e-04  3.82017719e-04  7.63099144e-04 -6.57585693e-05\n",
      "  1.99930506e-03 -1.48405048e-03 -7.37004406e-04  5.74669122e-04]\n",
      "\n",
      " [-0.00916125 -0.01561301 -0.02050438 -0.00687221 -0.02518709 -0.00900423\n",
      " -0.01306705 -0.00814856 -0.02851402 -0.00785542]\n",
      "\n",
      " [-0.04072659 -0.06857014 -0.09189647 -0.03046153 -0.09839169 -0.04038581\n",
      " -0.05904169 -0.03592379 -0.1168157  -0.0345206 ]\n",
      "weights, iteration 4\n",
      "[2.11966625e-05 2.87507923e-05 1.05983313e-05 0.00000000e+00]\n",
      "\n",
      " [ 0.04681828 -0.0379239   0.02490652  0.06770515]\n",
      "\n",
      " [-0.00549065 -0.00431746 -0.00312604 -0.00479582 -0.00425404 -0.00439156\n",
      " -0.00492481 -0.00516258 -0.00401136 -0.00414773 -0.0046061  -0.00406139\n",
      " -0.00440217 -0.00327036 -0.00437111 -0.00344651]\n",
      "\n",
      " [-0.50171362  0.1600247   0.47164126 -0.21683439 -0.1438269  -0.08592616\n",
      "  0.04584371 -0.33082483 -0.04875284 -0.06468524 -0.39035199  0.18812849\n",
      "  0.54603349  0.23759226  0.05883623 -0.08531281]\n",
      "biases, iteration 4\n",
      "\n",
      " [ 1.86437927e-04 -2.82808653e-04 -1.30309430e-04  2.55981549e-04\n",
      "  3.14436406e-04 -5.22746663e-05 -2.14518214e-04 -5.97293448e-05\n",
      " -1.20951378e-04  1.70736435e-04  2.28140312e-04  5.54387323e-05\n",
      "  5.11092798e-04 -3.02975938e-04 -1.23092290e-04  1.87181795e-04]\n",
      "\n",
      " [ 6.23511968e-04 -1.80972213e-03 -1.03460166e-03  1.07710460e-03\n",
      "  1.29027340e-03 -4.89907922e-04 -1.38138132e-03 -4.99857492e-04\n",
      " -9.91004977e-04  5.52754154e-04  9.91239456e-04 -1.03198370e-05\n",
      "  2.51039786e-03 -1.78702642e-03 -8.60096696e-04  7.61850917e-04]\n",
      "\n",
      " [-0.00853834 -0.01457428 -0.0189475  -0.00643848 -0.02511139 -0.00838453\n",
      " -0.01212523 -0.00766812 -0.02732024 -0.00741093]\n",
      "\n",
      " [-0.04926493 -0.08314442 -0.11084397 -0.03690001 -0.12350307 -0.04877034\n",
      " -0.07116692 -0.04359192 -0.14413594 -0.04193154]\n",
      "weights, iteration 5\n",
      "[2.21537315e-05 3.00489443e-05 1.10768657e-05 0.00000000e+00]\n",
      "\n",
      " [ 0.04684044 -0.03789385  0.0249176   0.06770515]\n",
      "\n",
      " [-0.0051208  -0.00407868 -0.00280065 -0.00455481 -0.00397829 -0.00413793\n",
      " -0.00460257 -0.00485892 -0.00369092 -0.00387484 -0.00435088 -0.00375532\n",
      " -0.00407009 -0.00299616 -0.00407801 -0.00312548]\n",
      "\n",
      " [-0.50683441  0.15594602  0.46884062 -0.2213892  -0.14780519 -0.09006409\n",
      "  0.04124114 -0.33568375 -0.05244376 -0.06856007 -0.39470287  0.18437317\n",
      "  0.54196341  0.2345961   0.05475822 -0.08843829]\n",
      "biases, iteration 5\n",
      "\n",
      " [ 2.04816903e-04 -2.47303717e-04 -9.84806599e-05  2.66472201e-04\n",
      "  3.32327057e-04 -3.15328887e-05 -1.77961719e-04 -3.51384503e-05\n",
      " -8.65077219e-05  1.89247833e-04  2.38140324e-04  7.87499957e-05\n",
      "  5.07492426e-04 -2.67773557e-04 -1.01113386e-04  2.02247849e-04]\n",
      "\n",
      " [ 8.28328870e-04 -2.05702585e-03 -1.13308232e-03  1.34357680e-03\n",
      "  1.62260045e-03 -5.21440811e-04 -1.55934304e-03 -5.34995942e-04\n",
      " -1.07751270e-03  7.42001986e-04  1.22937978e-03  6.84301587e-05\n",
      "  3.01789029e-03 -2.05479998e-03 -9.61210081e-04  9.64098767e-04]\n",
      "\n",
      " [-0.00795225 -0.01356225 -0.01749578 -0.00603606 -0.02472253 -0.00781948\n",
      " -0.01127764 -0.00722446 -0.02581556 -0.00700237]\n",
      "\n",
      " [-0.05721718 -0.09670667 -0.12833976 -0.04293607 -0.1482256  -0.05658982\n",
      " -0.08244456 -0.05081638 -0.16995151 -0.0489339 ]\n",
      "weights, iteration 6\n",
      "[2.27519547e-05 3.08603641e-05 1.13759774e-05 0.00000000e+00]\n",
      "\n",
      " [ 0.04686319 -0.03786299  0.02492897  0.06770515]\n",
      "\n",
      " [-0.00477386 -0.00385694 -0.00249742 -0.00432669 -0.00371667 -0.00390061\n",
      " -0.00430217 -0.0045757  -0.00339236 -0.0036176  -0.00411047 -0.0034678\n",
      " -0.00375249 -0.00274187 -0.00380475 -0.00282457]\n",
      "\n",
      " [-0.51160828  0.15208909  0.4663432  -0.22571589 -0.15152187 -0.0939647\n",
      "  0.03693896 -0.34025945 -0.05583611 -0.07217767 -0.39881335  0.18090537\n",
      "  0.53821092  0.23185423  0.05095347 -0.09126286]\n",
      "biases, iteration 6\n",
      "\n",
      " [ 2.17699571e-04 -2.14529386e-04 -7.01279394e-05  2.72240921e-04\n",
      "  3.44099859e-04 -1.25060060e-05 -1.41722961e-04 -1.06007588e-05\n",
      " -5.55334806e-05  2.02330276e-04  2.45205251e-04  9.86669444e-05\n",
      "  5.00004811e-04 -2.31677165e-04 -8.02204082e-05  2.15010491e-04]\n",
      "\n",
      " [ 0.00104603 -0.00227156 -0.00120321  0.00161582  0.0019667  -0.00053395\n",
      " -0.00170107 -0.0005456  -0.00113305  0.00094433  0.00147459  0.0001671\n",
      "  0.0035179  -0.00228648 -0.00104143  0.00117911]\n",
      "\n",
      " [-0.007403   -0.01259269 -0.01616004 -0.00566197 -0.02403372 -0.00730328\n",
      " -0.01051328 -0.00681354 -0.02412161 -0.00662601]\n",
      "\n",
      " [-0.06462018 -0.10929936 -0.1444998  -0.04859804 -0.17225932 -0.06389309\n",
      " -0.09295785 -0.05762991 -0.19407312 -0.05555991]\n",
      "weights, iteration 7\n",
      "[2.30531353e-05 3.12688803e-05 1.15265676e-05 0.00000000e+00]\n",
      "\n",
      " [ 0.04688624 -0.03783172  0.0249405   0.06770515]\n",
      "\n",
      " [-0.00444925 -0.00365137 -0.00221524 -0.00411135 -0.00346901 -0.00367906\n",
      " -0.00402258 -0.00431214 -0.00311459 -0.00337564 -0.00388468 -0.00319821\n",
      " -0.00344987 -0.00250628 -0.00355052 -0.00254316]\n",
      "\n",
      " [-0.51605753  0.14843772  0.46412795 -0.22982724 -0.15499087 -0.09764376\n",
      "  0.03291638 -0.34457159 -0.0589507  -0.07555331 -0.40269803  0.17770716\n",
      "  0.53476105  0.22934794  0.04740295 -0.09380602]\n",
      "biases, iteration 7\n",
      "\n",
      " [ 2.25919086e-04 -1.84726560e-04 -4.52333707e-05  2.73868189e-04\n",
      "  3.49913616e-04  4.51353461e-06 -1.07504065e-04  1.27367801e-05\n",
      " -2.85780765e-05  2.10625284e-04  2.49324858e-04  1.15147652e-04\n",
      "  4.89126570e-04 -1.96423970e-04 -6.05726688e-05  2.25120005e-04]\n",
      "\n",
      " [ 0.00127195 -0.00245628 -0.00124844  0.00188969  0.00231661 -0.00052943\n",
      " -0.00180857 -0.00053286 -0.00116162  0.00115496  0.00172391  0.00028224\n",
      "  0.00400702 -0.0024829  -0.001102    0.00140423]\n",
      "\n",
      " [-0.00688991 -0.01167562 -0.01494142 -0.00531371 -0.02308395 -0.00683091\n",
      " -0.00982239 -0.00643204 -0.02235274 -0.00627871]\n",
      "\n",
      " [-0.07151009 -0.12097498 -0.15944122 -0.05391175 -0.19534328 -0.070724\n",
      " -0.10278024 -0.06406195 -0.21642585 -0.06183862]\n",
      "weights, iteration 8\n",
      "[2.31159726e-05 3.13541118e-05 1.15579863e-05 0.00000000e+00]\n",
      "\n",
      " [ 0.04690936 -0.03780036  0.02495206  0.06770515]\n",
      "\n",
      " [-0.00414618 -0.00346102 -0.00195291 -0.00390857 -0.00323499 -0.00347259\n",
      " -0.00376257 -0.00406728 -0.00285642 -0.00314849 -0.00367311 -0.00294579\n",
      " -0.00316248 -0.00228809 -0.00331437 -0.00228044]\n",
      "\n",
      " [-0.52020371  0.1449767   0.46217504 -0.2337358  -0.15822586 -0.10111635\n",
      "  0.02915382 -0.34863887 -0.06180712 -0.0787018  -0.40637114  0.17476137\n",
      "  0.53159857  0.22705985  0.04408858 -0.09608647]\n",
      "biases, iteration 8\n",
      "\n",
      " [ 2.30311537e-04 -1.58000785e-04 -2.36957776e-05  2.72065643e-04\n",
      "  3.50353148e-04  1.93555834e-05 -7.64958373e-05  3.40298431e-05\n",
      " -5.85572199e-06  2.14887386e-04  2.50678772e-04  1.28320716e-04\n",
      "  4.75478859e-04 -1.63327399e-04 -4.23829761e-05  2.32462264e-04]\n",
      "\n",
      " [ 0.00150226 -0.00261428 -0.00127214  0.00216175  0.00266697 -0.00051008\n",
      " -0.00188507 -0.00049883 -0.00116748  0.00136984  0.00197459  0.00041057\n",
      "  0.0044825  -0.00264623 -0.00114439  0.00163669]\n",
      "\n",
      " [-0.00641183 -0.01081647 -0.01383535 -0.00498921 -0.0219315  -0.00639804\n",
      " -0.0091965  -0.00607725 -0.02060009 -0.00595777]\n",
      "\n",
      " [-0.07792192 -0.13179146 -0.17327656 -0.05890096 -0.21727478 -0.07712204\n",
      " -0.11197674 -0.0701392  -0.23702594 -0.06779639]\n",
      "weights, iteration 9\n",
      "[2.29911569e-05 3.11848139e-05 1.14955785e-05 0.00000000e+00]\n",
      "\n",
      " [ 0.04693235 -0.03776918  0.02496355  0.06770515]\n",
      "\n",
      " [-0.00386374 -0.00328491 -0.00170917 -0.003718   -0.00301427 -0.00328042\n",
      " -0.00352089 -0.00384005 -0.00261664 -0.00293559 -0.00347528 -0.00270971\n",
      " -0.00289031 -0.00208597 -0.00309526 -0.00203551]\n",
      "\n",
      " [-0.52406745  0.14169179  0.46046587 -0.2374538  -0.16124014 -0.10439677\n",
      "  0.02563293 -0.35247892 -0.06442376 -0.08163739 -0.40984642  0.17205166\n",
      "  0.52870825  0.22497388  0.04099332 -0.09812198]\n",
      "biases, iteration 9\n",
      "\n",
      " [ 2.31632212e-04 -1.34330190e-04 -5.34401905e-06  2.67573764e-04\n",
      "  3.46290271e-04  3.19762530e-05 -4.93360327e-05  5.27615892e-05\n",
      "  1.27332191e-05  2.15890720e-04  2.49591659e-04  1.38437891e-04\n",
      "  4.59721539e-04 -1.33217330e-04 -2.58553348e-05  2.37125824e-04]\n",
      "\n",
      " [ 0.00173389 -0.00274861 -0.00127748  0.00242933  0.00301326 -0.0004781\n",
      " -0.0019344  -0.00044607 -0.00115475  0.00158574  0.00222418  0.000549\n",
      "  0.00494222 -0.00277945 -0.00117024  0.00187382]\n",
      "\n",
      " [-0.00596728 -0.0100173  -0.01283431 -0.00468671 -0.02064385 -0.0060009\n",
      " -0.00862827 -0.00574692 -0.01892634 -0.00566086]\n",
      "\n",
      " [-0.0838892  -0.14180875 -0.18611088 -0.06358768 -0.23791863 -0.08312294\n",
      " -0.12060501 -0.07588612 -0.25595229 -0.07345725]\n"
     ]
    }
   ],
   "source": [
    "#Temporarily training on 10 data example for trouble shooting\n",
    "learning_rate = 0.1\n",
    "for w in range(1):\n",
    "    train_data = train_datas[w*10:(w+1)*10] \n",
    "    for o in range(10):\n",
    "        #global cost,z,a,one_hot\n",
    "        #global Zs,As\n",
    "        cost = 0\n",
    "\n",
    "        # Create temporary storage for averaging weights and biases\n",
    "        temp_weights = [] #list to store all the weights for every layer\n",
    "        temp_biases = [] #list to store all the biases for every layer\n",
    "\n",
    "        temp_weights.append(np.zeros(shape=(size[0],len(train_data[0])))) #First layer\n",
    "        temp_biases.append(np.zeros(size[0])) \n",
    "        for i in range(len(size)-1): \n",
    "            temp_weights.append(np.zeros(shape=(size[i+1],size[i]))) #following layers\n",
    "            temp_biases.append(np.zeros(size[i+1])) \n",
    "\n",
    "\n",
    "        for i in range(len(train_data)): #Iterate through every train_data\n",
    "            #Forward propagation\n",
    "            Zs = []\n",
    "            As = [train_data[i]] #TAKE NOTE that As and Zs will be different because we put in initial input as first item for QOL during backprop\n",
    "            z = weights[0] @ train_data[i] + biases[0] #First layer\n",
    "            a = sigmoid(z)\n",
    "            Zs.append(z) #Storing data for backward propagation\n",
    "            As.append(a)\n",
    "            \n",
    "            for j in range(len(size)-1): \n",
    "                z = weights[j+1] @ a + biases[j+1] #Following layers\n",
    "                a = sigmoid(z)\n",
    "                Zs.append(z) #Storing data for backward propagation\n",
    "                As.append(a)\n",
    "\n",
    "            #Calculating cost\n",
    "            \n",
    "            one_hot = np.zeros(10)\n",
    "            one_hot[train_label[i]]=1\n",
    "            \n",
    "            cost = cost + np.sum((a - one_hot)**2) #Just to keep track of model fit\n",
    "\n",
    "            #final/output layer Backpropagation\n",
    "            dC_da = 2*(a - one_hot) \n",
    "            #print(\"Last layer dC_da=\",dC_da,\"\\n\")\n",
    "            dadz = (np.exp(-z) / (1 + np.exp(-z))**2)\n",
    "            \n",
    "            for x in range (len(weights[-1][0])): #iterating through weights column by column\n",
    "                # updating weights              \n",
    "                dzdw = As[-2][x] #This one input, affects a whole column of weights\n",
    "                dC_dw = dC_da * dadz * dzdw \n",
    "\n",
    "                \n",
    "                (temp_weights[-1])[:,x] += -dC_dw*learning_rate/len(train_data) #keeping track of updates to the weights\n",
    "                \n",
    "                \n",
    "            #updating Biases\n",
    "            dzdb = 1\n",
    "            dC_db = dC_da * dadz * dzdb\n",
    "            temp_biases[-1] += -dC_db*(learning_rate)/len(train_data) #keeping track of updates to the biases\n",
    "\n",
    "            #print(\"Updates to biases=\", temp_biases[-1] ) #DEBUGGING\n",
    "\n",
    "            global dCda_0 \n",
    "            #Previous layer Backpropagation\n",
    "            dCda_0 = np.array([])\n",
    "            for x in range (len(weights[-1][0])): #iterating through inputs, a, summing weights column by column, \n",
    "                dzda_0 = weights[-1][:,x] #A whole column of weights affect how ONE prev layer input affects the next layer \n",
    "                dC_da_0 = np.sum(dC_da*dadz*dzda_0)/len(weights[-1]) #Keep track of how previous layer output affect next layer for chain rule later\n",
    "                dCda_0 = np.append(dCda_0,dC_da_0)\n",
    "            #print(\"second from last layer dCda=\\n\",dCda_0)\n",
    "\n",
    "            #Previous layer weights\n",
    "            for k in range(len(size)-1): #iterating through layers, starting from the second last\n",
    "                z = Zs[-k-2]\n",
    "                dadz = (np.exp(-z) / (1 + np.exp(-z))**2)\n",
    "                \n",
    "                #Updating previous layer weights\n",
    "                for l in range (len(weights[-2-k][0])): #iterating through weights column by column (-2-k because we start from second from last)\n",
    "                    \n",
    "                    dzdw = As[-3-k][l] #This one input, affects a whole column of weights\n",
    "                    dC_dw = dCda_0 * dadz * dzdw\n",
    "                    \n",
    "                    (temp_weights[-2-k])[:,l] += -dC_dw*(learning_rate)/len(train_data) #keeping track of updates to the weights\n",
    "\n",
    "\n",
    "                #updating Biases\n",
    "                dzdb = 1\n",
    "                dC_db = dCda_0 * dadz * dzdb\n",
    "                temp_biases[-2-k] += -dC_db*(learning_rate)/len(train_data) #keeping track of updates to the biases\n",
    "\n",
    "                #Keep track of how this layer output affect next layer for chain rule later\n",
    "                temp_dCda_0 = np.array([])\n",
    "                for x in range (len(weights[-2-k][0])): #iterating through inputs, a, summing weights column by column\n",
    "                    dzda_0 = weights[-2-k][:,x] #A whole column of weights affect how ONE prev layer input affects the next layer \n",
    "                    dC_da_0 = np.sum(dCda_0*dadz*dzda_0)/len(weights[-2-k]) \n",
    "                    temp_dCda_0 = np.append(temp_dCda_0,dC_da_0)\n",
    "                \n",
    "                dCda_0 = temp_dCda_0 #MUtable / unmutable object? Is this going to be problem?\n",
    "\n",
    "        #Updating biases and weights\n",
    "\n",
    "        for i in range(len(size)):\n",
    "            weights[i] += temp_weights[i]\n",
    "            biases[i] += temp_biases[i]\n",
    "\n",
    "        # Analysis of changes to weights \n",
    "        print(\"weights, iteration\",o)\n",
    "        print(temp_weights[0][0][132:136])\n",
    "\n",
    "        print(\"\\n\", weights[0][0][132:136])\n",
    "\n",
    "        print(\"\\n\",temp_weights[1][0])\n",
    "\n",
    "        print(\"\\n\", weights[1][0])\n",
    "\n",
    "        # Analysis of changes to biases \n",
    "        print(\"biases, iteration\",o)\n",
    "        print(\"\\n\",temp_biases[0])\n",
    "\n",
    "        print(\"\\n\", biases[0])\n",
    "\n",
    "        print(\"\\n\", temp_biases[1])\n",
    "\n",
    "        print(\"\\n\", biases[1])\n",
    "\n",
    "        \n",
    "                \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26140462",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14.08779937291723"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8967f33a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First layer, \n",
      "z= [ 0.59352671 -0.07510365  0.00176616  0.32869294 -0.03007599 -0.27031898\n",
      "  0.13036989  0.27099842  0.36335949 -0.28867033  0.2827137   0.1660659\n",
      " -0.03329005 -0.36498986  0.09094213  0.21033653] \n",
      "a= [0.64417393 0.48123291 0.50044154 0.58144131 0.49248157 0.43282879\n",
      " 0.53254639 0.567338   0.58985343 0.42832942 0.5702114  0.54142133\n",
      " 0.49167826 0.4097522  0.52271987 0.55239112]\n",
      "\n",
      " 1 th layer, \n",
      "z= [-0.46599064 -0.34925698 -0.9116827  -0.87353634 -0.06009452 -1.09594521\n",
      " -1.19446317 -1.1359905  -0.52587584 -1.46083889] \n",
      "a= [0.38556564 0.41356261 0.28665563 0.294519   0.48498089 0.25050041\n",
      " 0.23246165 0.24305727 0.37147929 0.18833905]\n",
      "\n",
      "evaluation= [0.38556564 0.41356261 0.28665563 0.294519   0.48498089 0.25050041\n",
      " 0.23246165 0.24305727 0.37147929 0.18833905] max=  4  label=  1\n"
     ]
    }
   ],
   "source": [
    "#Forward propagation, testing training fit\n",
    "m=0\n",
    "z = weights[0] @ train_datas[m] + biases[0] #First layer\n",
    "a = sigmoid(z)\n",
    "print(\"\\nFirst layer, \\nz=\",z,\"\\na=\",a )\n",
    "\n",
    "for j in range(len(size)-1): \n",
    "    z = weights[j+1] @ a + biases[j+1] #Following layers\n",
    "    a = sigmoid(z)\n",
    "    print(\"\\n\",j+1,\"th layer, \\nz=\",z,\"\\na=\",a )\n",
    "\n",
    "print(\"\\nevaluation=\",a,\"max= \",np.argmax(a),\" label= \",train_labels[m])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dddb7f50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First layer, \n",
      "z= [ 0.57247577 -0.88956797  0.91471418 -0.54673857  0.05350222 -0.61355624\n",
      "  0.14600472 -0.2834494   0.56115173  0.02980281 -0.53781147  0.28410036\n",
      "  0.50838543  0.1264589  -0.00230475  0.93432693] \n",
      "a= [0.63933425 0.29119899 0.71396386 0.36662142 0.51337237 0.3512484\n",
      " 0.53643648 0.42960831 0.63671899 0.50745015 0.36869684 0.57055119\n",
      " 0.62442791 0.53157266 0.49942381 0.7179523 ]\n",
      "\n",
      " 1 th layer, \n",
      "z= [-0.12874073 -0.49599182 -0.94756282 -0.79446769 -0.0513548  -1.09239514\n",
      " -1.31056131 -1.14918699 -0.51008878 -1.6286352 ] \n",
      "a= [0.4678592  0.37848307 0.27937522 0.31121018 0.48716412 0.25116753\n",
      " 0.21239293 0.24063761 0.37517271 0.16401741]\n",
      "\n",
      "evaluation= [0.4678592  0.37848307 0.27937522 0.31121018 0.48716412 0.25116753\n",
      " 0.21239293 0.24063761 0.37517271 0.16401741] max=  4  label=  0\n"
     ]
    }
   ],
   "source": [
    "#Forward propagation, testing training fit\n",
    "m=4\n",
    "z = weights[0] @ train_datas[m] + biases[0] #First layer\n",
    "a = sigmoid(z)\n",
    "print(\"\\nFirst layer, \\nz=\",z,\"\\na=\",a )\n",
    "\n",
    "for j in range(len(size)-1): \n",
    "    z = weights[j+1] @ a + biases[j+1] #Following layers\n",
    "    a = sigmoid(z)\n",
    "    print(\"\\n\",j+1,\"th layer, \\nz=\",z,\"\\na=\",a )\n",
    "\n",
    "print(\"\\nevaluation=\",a,\"max= \",np.argmax(a),\" label= \",train_labels[m])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9251b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "#Check accuracy on training set\n",
    "correct = 0\n",
    "k = 10\n",
    "for i in range(k):\n",
    "    z = weights[0] @ train_datas[i] + biases[0] #First layer\n",
    "    a = sigmoid(z)\n",
    "\n",
    "    for j in range(len(size)-1): \n",
    "        z = weights[j+1] @ a + biases[j+1] #Following layers\n",
    "        a = sigmoid(z)\n",
    "\n",
    "    if train_labels[i] == np.argmax(a): #np.argmax(a)\n",
    "        correct += 1\n",
    "        \n",
    "print(correct/k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b594cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
